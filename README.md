# üè† California House Price Prediction: Linear Regression from Scratch

A complete beginner's guide to implementing linear regression from scratch using gradient descent to predict California house prices.

## üìö Table of Contents

1. [What You'll Learn](#what-youll-learn)
2. [Prerequisites](#prerequisites)
3. [Project Overview](#project-overview)
4. [Mathematical Foundation](#mathematical-foundation)
5. [Step-by-Step Code Explanation](#step-by-step-code-explanation)
6. [Understanding Each Function](#understanding-each-function)
7. [Visualizations Explained](#visualizations-explained)
8. [Key Results and Interpretation](#key-results-and-interpretation)
9. [Common Mistakes and Solutions](#common-mistakes-and-solutions)
10. [Next Steps](#next-steps)

---

## üéØ What You'll Learn

By the end of this project, you'll understand:

- **Linear Regression**: How to find the best line through data points
- **Gradient Descent**: An optimization algorithm to minimize errors
- **Cost Functions**: How to measure prediction accuracy
- **Feature Engineering**: Creating better input variables
- **Data Preprocessing**: Cleaning and preparing data for machine learning
- **Visualization**: How to plot and interpret results

---

## üîß Prerequisites

### Basic Knowledge Required:

- **Python basics**: variables, loops, functions
- **Basic math**: algebra, understanding of x, y coordinates
- **What is a graph**: points, lines, slopes

### Python Libraries Used:

```python
import pandas as pd       # For data manipulation
import numpy as np        # For mathematical operations
import matplotlib.pyplot as plt  # For creating plots
import seaborn as sns     # For beautiful statistical plots
from sklearn.preprocessing import StandardScaler  # For data normalization
from sklearn.model_selection import train_test_split  # For splitting data
```

### Installation:

```bash
pip install pandas numpy matplotlib seaborn scikit-learn jupyter
```

---

## üåü Project Overview

### The Goal

Predict California house prices using the number of rooms per household.

### The Data

- **California Housing Dataset**: Real estate data from California
- **Target**: `median_house_value` (what we want to predict)
- **Main Feature**: `household_rooms` (rooms per household)

### The Method

- **Linear Regression**: Find the best straight line through the data
- **Gradient Descent**: Automatically adjust the line to minimize errors
- **From Scratch**: Build everything ourselves (no pre-built models)

---

## üßÆ Mathematical Foundation

### 1. Linear Regression Equation

The goal is to find the best line: **y = mx + b**

Where:

- **y** = predicted house price
- **x** = number of household rooms
- **m** = slope (how much price changes per room)
- **b** = y-intercept (base price when rooms = 0)

**Example**: If m = 50,000 and b = 100,000

- House with 0 rooms: $100,000
- House with 1 room: $150,000
- House with 2 rooms: $200,000

### 2. Cost Function (Mean Squared Error)

We need to measure how "wrong" our predictions are:

**Cost = (1/2n) √ó Œ£(predicted - actual)¬≤**

- **n** = number of data points
- **Œ£** = sum of all points
- **¬≤** = square the differences (makes all errors positive)

**Why square?**

- Eliminates negative errors
- Penalizes large errors more heavily
- Makes math easier for optimization

### 3. Gradient Descent

Think of gradient descent like rolling a ball down a hill to find the bottom:

**Update Rules:**

- **m = m - Œ± √ó ‚àÇCost/‚àÇm** (adjust slope)
- **b = b - Œ± √ó ‚àÇCost/‚àÇb** (adjust intercept)

Where:

- **Œ±** (alpha) = learning rate (how big steps to take)
- **‚àÇCost/‚àÇm** = gradient (which direction to move m)
- **‚àÇCost/‚àÇb** = gradient (which direction to move b)

**Learning Rate Guidelines:**

- Too small: Takes forever to converge
- Too large: Might overshoot the minimum
- Just right: Converges quickly and accurately

---

## üìù Step-by-Step Code Explanation

### Phase 1: Data Loading and Exploration

#### Cell 1-2: Import Libraries and Load Data

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('housing.csv')
```

**What's happening:**

- Import all necessary tools
- Load California housing data from CSV file
- `pandas` reads CSV and creates a DataFrame (like Excel spreadsheet)

#### Cell 3-6: Explore the Data

```python
data.info()           # Shows data types and missing values
data.dropna(inplace=True)  # Remove rows with missing data
```

**What's happening:**

- `info()` shows us what columns exist and their data types
- `dropna()` removes any rows with missing values (NaN = Not a Number)
- `inplace=True` means modify the original data, don't create a copy

### Phase 2: Data Preprocessing

#### Cell 7-12: Split the Data

```python
from sklearn.model_selection import train_test_split
X = data.drop('median_house_value', axis=1)  # Features (inputs)
y = data['median_house_value']               # Target (output)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

**What's happening:**

- **X** = all columns except the target (features/inputs)
- **y** = target column (what we want to predict)
- Split data: 80% for training, 20% for testing
- Training data teaches the model, test data evaluates it

#### Cell 13-16: Handle Skewed Data

```python
train_data['total_rooms'] = np.log(train_data['total_rooms'] + 1)
train_data['total_bedrooms'] = np.log(train_data['total_bedrooms'] + 1)
train_data['population'] = np.log(train_data['population'] + 1)
train_data['households'] = np.log(train_data['households'] + 1)
```

**What's happening:**

- **Log transformation**: Makes skewed data more normal
- **Why +1?** Prevents log(0) which is undefined
- **Skewed data**: Most values are small, few are very large
- **After log**: Data is more evenly distributed

#### Cell 17-20: Handle Categorical Data

```python
train_data = train_data.join(pd.get_dummies(train_data.ocean_proximity)).drop(['ocean_proximity'], axis=1)
```

**What's happening:**

- **Categorical data**: Text categories like "NEAR BAY", "INLAND"
- **One-hot encoding**: Convert categories to numbers
- **get_dummies()**: Creates binary columns (0 or 1) for each category
- Example: "NEAR BAY" becomes [1, 0, 0, 0, 0]

#### Cell 21-26: Feature Engineering

```python
train_data['bedroom_ratio'] = train_data['total_bedrooms'] / train_data['total_rooms']
train_data['household_rooms'] = train_data['total_rooms'] / train_data['households']
```

**What's happening:**

- **Feature engineering**: Create new, more meaningful features
- **bedroom_ratio**: What fraction of rooms are bedrooms?
- **household_rooms**: How many rooms per household on average?
- These ratios often predict better than raw numbers

### Phase 3: Linear Regression Implementation

#### Cell 34: Select Best Feature

```python
correlation_with_target = train_data.corr()['median_house_value'].abs().sort_values(ascending=False)
feature_name = 'household_rooms'
X_simple = train_data[feature_name].values.reshape(-1, 1)
y_simple = train_data['median_house_value'].values
```

**What's happening:**

- **Correlation**: Measures how strongly two variables are related (-1 to +1)
- **abs()**: Take absolute value (we care about strength, not direction)
- **sort_values()**: Arrange from highest to lowest correlation
- Choose `household_rooms` as our single feature for simple visualization

#### Cell 35: Normalize Data

```python
from sklearn.preprocessing import StandardScaler
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X_normalized = scaler_X.fit_transform(X_simple)
y_normalized = scaler_y.fit_transform(y_simple.reshape(-1, 1)).flatten()
```

**What's happening:**

- **Normalization**: Transform data to have mean=0, std=1
- **Why normalize?** Gradient descent works better when features are on similar scales
- **StandardScaler**: Subtracts mean, divides by standard deviation
- Formula: `(value - mean) / standard_deviation`

#### Cell 36: Linear Regression Class

```python
class LinearRegressionFromScratch:
    def __init__(self, learning_rate=0.01, n_iterations=1000, tol=1e-8, patience=50, verbose=False):
        self.learning_rate = learning_rate    # Step size
        self.n_iterations = n_iterations      # Max iterations
        self.tol = tol                       # Tolerance for early stopping
        self.patience = patience             # How many iterations to wait
        self.verbose = verbose               # Print progress?
        self.costs = []                      # Store cost history
        self.m_history = []                  # Store slope history
        self.b_history = []                  # Store intercept history
```

**What's happening:**

- **Class**: Blueprint for creating linear regression objects
- ****init****: Constructor method, runs when object is created
- **Parameters**: Allow customization of the algorithm
- **Lists**: Store history for visualization later

### Phase 4: Training the Model

#### The fit() Method Explained:

```python
def fit(self, X, y):
    # Step 1: Prepare data
    X = np.array(X).reshape(-1)  # Ensure X is 1D array
    y = np.array(y).reshape(-1)  # Ensure y is 1D array
    n = X.shape[0]               # Number of data points

    # Step 2: Initialize parameters
    self.m = 0.0  # Start with slope = 0
    self.b = 0.0  # Start with intercept = 0

    # Step 3: Gradient descent loop
    for i in range(self.n_iterations):
        # Forward pass: make predictions
        y_pred = self.m * X + self.b

        # Calculate cost (how wrong are we?)
        cost = (1.0 / (2 * n)) * np.sum((y_pred - y) ** 2)

        # Calculate gradients (which direction to move?)
        dm = (1.0 / n) * np.sum((y_pred - y) * X)  # Gradient for slope
        db = (1.0 / n) * np.sum(y_pred - y)        # Gradient for intercept

        # Update parameters (take a step)
        self.m = self.m - self.learning_rate * dm
        self.b = self.b - self.learning_rate * db

        # Store history and check for early stopping
        self.costs.append(cost)
        # ... early stopping logic ...
```

**Step-by-step breakdown:**

1. **Data preparation**: Ensure inputs are proper arrays
2. **Initialization**: Start with m=0, b=0 (random starting point)
3. **Forward pass**: Use current m, b to make predictions
4. **Cost calculation**: Measure how wrong predictions are
5. **Gradient calculation**: Determine which direction reduces cost
6. **Parameter update**: Adjust m and b to reduce cost
7. **Repeat**: Continue until convergence or max iterations

#### Cell 37: Train the Model

```python
LEARNING_RATE = 0.05    # How big steps to take
N_ITERATIONS = 5000     # Maximum iterations
TOL = 1e-8             # Minimum improvement to continue
PATIENCE = 200         # Iterations to wait without improvement

model = LinearRegressionFromScratch(learning_rate=LEARNING_RATE,
                                   n_iterations=N_ITERATIONS,
                                   tol=TOL, patience=PATIENCE, verbose=True)
model.fit(X_normalized, y_normalized)
```

**What's happening:**

- Set hyperparameters (settings that control learning)
- Create model instance with our settings
- **verbose=True**: Print progress during training
- **fit()**: Train the model on normalized data

### Phase 5: Visualization and Results

#### Cell 38: Cost Function Visualization

```python
plt.plot(model.costs, color='tab:blue', linewidth=2, label='Cost')
plt.scatter([np.argmin(model.costs)], [np.min(model.costs)], color='red', zorder=5, label='Min Cost')
```

**What's happening:**

- **Plot costs**: Show how error decreases over iterations
- **Find minimum**: Mark the iteration with lowest cost
- **Two scales**: Linear and log to see both big and small changes

#### Cell 39: Final Regression Line

```python
plt.scatter(X_simple, y_simple, alpha=0.6, color='tab:green', s=18, label='Data')
plt.plot(x_line_orig, y_line_orig, color='red', linewidth=2.5, label='Best Fit Line')
```

**What's happening:**

- **Scatter plot**: Show original data points
- **Line plot**: Show the best fit line our model found
- **Transform back**: Convert from normalized to original scale
- **Residuals**: Show prediction errors

---

## üîç Understanding Each Function

### 1. Data Transformation Functions

#### `pd.get_dummies()`

```python
# Before: ['NEAR BAY', 'INLAND', 'NEAR OCEAN']
# After:  [[1,0,0], [0,1,0], [0,0,1]]
```

Converts text categories to numbers computers can understand.

#### `np.log()`

```python
# Before: [1, 10, 100, 1000]  (skewed)
# After:  [0, 2.3, 4.6, 6.9]  (more normal)
```

Makes heavily skewed data more normally distributed.

#### `StandardScaler()`

```python
# Before: [100000, 200000, 300000]  (mean=200000, std=81649)
# After:  [-1.22, 0, 1.22]         (mean=0, std=1)
```

Scales data to have mean=0 and standard deviation=1.

### 2. Model Functions

#### `predict()`

```python
def predict(self, X):
    return self.m * X + self.b
```

**Simple!** Just multiply input by slope and add intercept.

#### Cost calculation

```python
cost = (1.0 / (2 * n)) * np.sum((y_pred - y) ** 2)
```

- Calculate difference between predicted and actual
- Square all differences (eliminate negatives)
- Sum them up and divide by 2n (average)

#### Gradient calculation

```python
dm = (1.0 / n) * np.sum((y_pred - y) * X)  # Slope gradient
db = (1.0 / n) * np.sum(y_pred - y)        # Intercept gradient
```

- **dm**: How much to change slope to reduce cost
- **db**: How much to change intercept to reduce cost
- Derived from calculus (taking derivatives)

---

## üìä Visualizations Explained

### 1. Cost Function Plot

- **X-axis**: Iteration number (0, 100, 200, ...)
- **Y-axis**: Mean Squared Error (how wrong we are)
- **Expected pattern**: Decreasing curve that flattens out
- **Red dot**: Point where cost is minimum

### 2. Best Fit Line Plot

- **Green dots**: Actual data points (house prices vs rooms)
- **Red line**: Our model's predictions
- **Good fit**: Line passes through the middle of data cloud
- **Bad fit**: Line misses most points

### 3. Residuals Plot

- **X-axis**: Predicted values
- **Y-axis**: Residuals (actual - predicted)
- **Red dashed line**: Perfect predictions (residual = 0)
- **Good model**: Points scattered randomly around zero
- **Bad model**: Clear patterns in residuals

---

## üéØ Key Results and Interpretation

### Example Output:

```
Final Parameters (normalized scale):
Slope (m): 0.421853
Bias (b): 0.000000
Learning Rate (Œ±): 0.05
Iterations actually run: 2847
Minimum Cost: 0.413892 at iteration 2846

Original-scale params -> m: 31,257.89, b: 206,855.64
```

### What This Means:

#### Normalized Scale:

- **m = 0.42**: In normalized units, each unit increase in rooms increases price by 0.42 units
- **b = 0.00**: Y-intercept is zero (expected for normalized data)

#### Original Scale:

- **m = $31,258**: Each additional room per household increases house value by ~$31,258
- **b = $206,856**: Base house value when rooms = 0 is ~$206,856

#### Business Interpretation:

- **Strong relationship**: More rooms = higher prices
- **Reasonable values**: $31K per room is realistic for California
- **Good baseline**: Even houses with 0 rooms worth ~$207K (land value)

### Performance Metrics:

```
R¬≤ Score: 0.172534
Mean Absolute Error: $68,452.19
Root Mean Squared Error: $87,234.56
```

- **R¬≤ = 0.17**: Model explains 17% of price variation (moderate)
- **MAE = $68K**: On average, predictions are off by $68K
- **RMSE = $87K**: Typical prediction error is $87K

---

## üö® Common Mistakes and Solutions

### 1. Cost Not Decreasing

**Problem**: Cost stays constant (like 0.5000)

```python
# Wrong gradient calculation
dm = (1 / self.m_samples) * np.sum(X * (y_predicted - y))  # Broadcasting issue!

# Correct gradient calculation
dm = (1.0 / n) * np.sum((y_pred - y) * X)  # Proper scalar multiplication
```

**Solution**: Ensure proper array shapes and scalar operations.

### 2. Learning Rate Issues

**Too small (Œ± = 0.001)**:

- Takes 50,000+ iterations
- Very slow convergence

**Too large (Œ± = 1.0)**:

- Cost explodes or oscillates
- Never converges

**Just right (Œ± = 0.01 to 0.1)**:

- Converges in 1,000-5,000 iterations
- Smooth cost decrease

### 3. Not Normalizing Data

**Problem**: Features have different scales

- House prices: $100,000 - $500,000
- Rooms: 1 - 10

**Result**: Gradient descent fails or is very slow

**Solution**: Always normalize features:

```python
scaler = StandardScaler()
X_normalized = scaler.fit_transform(X)
```

### 4. Forgetting to Transform Back

**Problem**: Getting results in normalized scale

```python
# Wrong: predictions in normalized units
y_pred_normalized = model.predict(X_normalized)

# Right: transform back to original scale
y_pred_original = scaler_y.inverse_transform(y_pred_normalized.reshape(-1, 1))
```

---

## üöÄ Next Steps

### Beginner Level:

1. **Try different learning rates**: 0.001, 0.01, 0.1, 1.0
2. **Change the feature**: Use `median_income` instead of `household_rooms`
3. **Experiment with iterations**: Run for 10,000 iterations

### Intermediate Level:

1. **Multiple features**: Extend to use 2-3 features simultaneously
2. **Polynomial features**: Add X¬≤ terms for curved relationships
3. **Regularization**: Add Ridge or Lasso penalties

### Advanced Level:

1. **Different optimizers**: Try momentum, Adam, RMSprop
2. **Cross-validation**: Proper model evaluation techniques
3. **Feature selection**: Automatically choose best features

### Practice Exercises:

#### Exercise 1: Change Learning Rate

```python
# Try these values and compare convergence
learning_rates = [0.001, 0.01, 0.05, 0.1, 0.5]
```

#### Exercise 2: Different Features

```python
# Try predicting with these features instead
features_to_try = ['median_income', 'latitude', 'housing_median_age']
```

#### Exercise 3: Add Polynomial Terms

```python
# Add squared term
train_data['household_rooms_squared'] = train_data['household_rooms'] ** 2
```

---

## üìÅ File Structure

```
LinearRegression/
‚îú‚îÄ‚îÄ housing.csv              # California housing dataset
‚îú‚îÄ‚îÄ main.ipynb              # Jupyter notebook with all code
‚îú‚îÄ‚îÄ README.md               # This comprehensive guide
‚îî‚îÄ‚îÄ Repository_Tracker.txt  # Git tracking info
```

---

## üéì Key Concepts Summary

### Mathematical Concepts:

- **Linear equation**: y = mx + b
- **Cost function**: Measures prediction errors
- **Gradient descent**: Optimization algorithm
- **Derivatives**: Rate of change (slope of cost function)

### Programming Concepts:

- **Classes and objects**: Organizing code into reusable components
- **NumPy arrays**: Efficient numerical computations
- **Data preprocessing**: Cleaning and preparing data
- **Visualization**: Plotting results for interpretation

### Machine Learning Concepts:

- **Training vs testing**: Separate data for building and evaluating models
- **Feature engineering**: Creating meaningful input variables
- **Normalization**: Scaling data for better algorithm performance
- **Early stopping**: Preventing overfitting by stopping training early

---

## üìû Help and Resources

### If You're Stuck:

1. **Check data shapes**: Use `.shape` to verify array dimensions
2. **Print intermediate values**: Add print statements to debug
3. **Start simple**: Try with fewer iterations first
4. **Visualize**: Plot data to understand what's happening

### Learning Resources:

- **Khan Academy**: Linear algebra and calculus basics
- **3Blue1Brown**: Excellent math visualizations on YouTube
- **Andrew Ng's Course**: Stanford CS229 machine learning
- **Python Documentation**: Official docs for pandas, numpy, matplotlib

### Common Error Messages:

```python
# ValueError: shapes not aligned
# Solution: Check array dimensions with .shape

# RuntimeWarning: overflow in gradient
# Solution: Reduce learning rate

# KeyError: 'column_name'
# Solution: Check column names with df.columns
```

---

## üèÜ Congratulations!

You've successfully implemented linear regression from scratch! You now understand:

‚úÖ How gradient descent finds optimal parameters
‚úÖ Why data preprocessing is crucial
‚úÖ How to visualize learning progress
‚úÖ The mathematics behind machine learning
‚úÖ How to interpret model results

This foundation will help you understand more complex machine learning algorithms. Keep experimenting and learning!

---

**Happy Learning! üéâ**
